# Architectural Basics

  1.	Kernels and how do we decide the number of kernels?
  2.	3x3 Convolutions,
  3.	How many layers,
  4.	Receptive Field,
  5.	MaxPooling,
  6.	Position of MaxPooling,
  7.	1x1 Convolutions,
  8.	SoftMax,
  9.	Activation Functions
  10.	Why do we need non-linearities?
  11.	Types of Activation function
  12.	Fully connected,
  13.	Learning Rate,
  14.	Batch Normalization
  15.	The distance of Batch Normalization from Prediction,
  16.	Concept of Transition Layers,
  17.	Position of Transition Layer,
  18.	Batch Normalization,
  19.	Image Normalization,
  20.	Number of Epochs and when to increase them,
  21.	DropOut
  22.	When do we introduce DropOut, or when do we know we have some overfitting,
  23.	The distance of MaxPooling from Prediction,
  24.	How do we know our network is not going well, comparatively, very early
  25.	Batch Size, and effects of batch size
  26.	When to add validation checks
  27.	LR schedule and concept behind it
  28.	Adam vs. SGD
  29.	When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)


  
