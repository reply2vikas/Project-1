{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st DNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reply2vikas/Project-1/blob/master/Assignment_3_1st_DNN-VK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "2730a0d0-c198-4d66-bbc6-a84c9f31c0ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Import necessary libraries and Installing packages\n",
        "# https://keras.io/\n",
        "\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing necessary libraries and packages from Keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the dataset\n",
        "#load_data() returns two tuples of Numpy arrays. The first tuple represents the training x-y pairs while the second\n",
        "#tuple represents the testing x-y pairs.\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "2b4cb1d4-522c-4155-9354-b60e1631f828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "#Printing the Mnist image with shape (It is a tuple of integers that describe how many dimensions the tensor has\n",
        "#along each axis. In the Numpy library this attribute is called shape.)\n",
        "#Importing the pyplot package to show the Mnist image\n",
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5937138a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshapping the data (Verifing that x_train.shape takes the form of (60000, 784) and x_test.shape takes the\n",
        "#form of (10000, 784), where the first dimension indexes the image and the second indexes the pixel in each \n",
        "#image now the intensity of the pixel is a value between 0 and 1))\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#These MNIST images of 28×28 pixels are represented as an array of numbers whose values range from [0, 255] \n",
        "#of type uint8. But it is usual to scale the input values of neural networks to certain ranges. Here the input values \n",
        "#should be scaled to values of type float32 within the interval [0, 1]. We can achieve this transformation with the\n",
        "#following lines of code:\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "bf4a55a3-b1d9-4601-8c1f-fd0152be84a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#To see the effect of the transformation we can see the values before and after applying to_categorical:-\n",
        "\n",
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "# In this example we will use what is known as one-hot encoding, which we have already mentioned, which consists\n",
        "# of transforming the labels into a vector of as many zeros as the number of different labels, and containing the \n",
        "# value of 1 in the index that corresponds to the value of the label. Keras offers many support functions, including \n",
        "# to_categorical to perform precisely this transformation, which we can import from keras.utils-\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "9673d8bd-6a29-433b-d190-35d8dce19395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Now we will do data ready to be used in our simple model example that we are going to program in Keras in the \n",
        "#next section.\n",
        "\n",
        "Y_train[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "4f4f2f6b-8260-47c1-cf65-4514f11fd069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "#Defining the Model\n",
        "#Importing Activation and MaxPooling2D methos from Library/Package Keras\n",
        "# import BatchNormalization\n",
        "\n",
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(64, 3, 3, activation='relu', input_shape=(28,28,1))) # 26\n",
        "#model.add(keras.layers.Dropout(0.1))\n",
        "model.add(Convolution2D(16, 1, activation='relu'))\n",
        "model.add(Convolution2D(16, 3,3, activation='relu')) # 24\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #12\n",
        "\n",
        "model.add(Convolution2D(32, 3,3, activation='relu')) # 10\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(Convolution2D(16, 3,3, activation='relu')) # 8\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #4\n",
        "\n",
        "\n",
        "\n",
        "model.add(Convolution2D(32, 3,3, activation='relu')) # 2\n",
        "model.add(keras.layers.Dropout(0.1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(10, 1))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "print(\"Params = \",model.count_params())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=\"adam\",\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=2048, epochs=80, verbose=0)\n",
        "score = model.evaluate(X_train, Y_train, verbose=0)\n",
        "print(\"Train Score = \",score)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Score = \",score)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0808 19:37:29.349390 140022408750976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "W0808 19:37:29.364433 140022408750976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0808 19:37:29.367105 140022408750976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "W0808 19:37:29.406450 140022408750976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "W0808 19:37:29.423056 140022408750976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0808 19:37:29.431565 140022408750976 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "W0808 19:37:29.516301 140022408750976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0808 19:37:29.539914 140022408750976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Params =  18234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0808 19:37:29.739536 140022408750976 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Score =  [0.012063788315762455, 0.9968833333333333]\n",
            "Score =  [0.022670664560998557, 0.9928]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "d63ecb87-0027-45b5-ad85-d43c27a8c9ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "#A very useful method that Keras provides to check the architecture of our model is summary()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 16)        1040      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 32)        4640      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 16)          4624      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 2, 2, 32)          4640      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 1, 10)          330       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,234\n",
            "Trainable params: 18,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Configuration of the learning process\n",
        "#we are specify the following arguments in compile() method to test it on our computer:\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "6291494c-1555-4ef1-b1e1-7b97eaa8def5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "#Model training\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=23, verbose=1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/23\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0556 - acc: 0.9822\n",
            "Epoch 2/23\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.0471 - acc: 0.9853\n",
            "Epoch 3/23\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.0409 - acc: 0.9871\n",
            "Epoch 4/23\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0390 - acc: 0.9880\n",
            "Epoch 5/23\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.0350 - acc: 0.9892\n",
            "Epoch 6/23\n",
            "60000/60000 [==============================] - 12s 194us/step - loss: 0.0311 - acc: 0.9902\n",
            "Epoch 7/23\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0282 - acc: 0.9912\n",
            "Epoch 8/23\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0261 - acc: 0.9918\n",
            "Epoch 9/23\n",
            "60000/60000 [==============================] - 11s 192us/step - loss: 0.0262 - acc: 0.9917\n",
            "Epoch 10/23\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0243 - acc: 0.9925\n",
            "Epoch 11/23\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0235 - acc: 0.9923\n",
            "Epoch 12/23\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0221 - acc: 0.9926\n",
            "Epoch 13/23\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0208 - acc: 0.9930\n",
            "Epoch 14/23\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.0193 - acc: 0.9938\n",
            "Epoch 15/23\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.0200 - acc: 0.9933\n",
            "Epoch 16/23\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0181 - acc: 0.9942\n",
            "Epoch 17/23\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.0186 - acc: 0.9941\n",
            "Epoch 18/23\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0170 - acc: 0.9944\n",
            "Epoch 19/23\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.0160 - acc: 0.9947\n",
            "Epoch 20/23\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.0163 - acc: 0.9945\n",
            "Epoch 21/23\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0151 - acc: 0.9951\n",
            "Epoch 22/23\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0161 - acc: 0.9949\n",
            "Epoch 23/23\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0152 - acc: 0.9948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5933b41be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model evaluation\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "1ac7ad92-7aef-4582-f403-ba02bbed887b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.023916802410105447, 0.9937]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "6533076f-8c83-4817-bd18-409415388ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.91949528e-11 1.99256633e-09 2.94697202e-07 8.25698510e-10\n",
            "  7.77265530e-10 1.44824933e-11 8.73145617e-20 9.99999762e-01\n",
            "  4.36173233e-12 1.61251972e-08]\n",
            " [2.44672815e-09 1.07637259e-08 1.00000000e+00 5.03792119e-10\n",
            "  8.61274219e-10 6.63223654e-13 6.94262148e-11 2.37233220e-13\n",
            "  3.40252893e-09 1.68306605e-10]\n",
            " [3.92734364e-07 9.99943972e-01 3.49766037e-06 7.04277667e-08\n",
            "  2.22204544e-05 9.48616162e-06 8.48846810e-07 1.36760154e-05\n",
            "  3.19467017e-06 2.59996432e-06]\n",
            " [9.99998450e-01 8.09488414e-12 8.00387383e-08 2.00433163e-08\n",
            "  3.00273778e-10 9.49474241e-08 5.40863368e-07 1.23449662e-11\n",
            "  8.28788075e-07 8.50117932e-09]\n",
            " [1.80199909e-12 6.53658727e-09 5.65081648e-10 6.92970732e-14\n",
            "  9.99999404e-01 1.59811197e-13 3.57063554e-13 3.82603060e-09\n",
            "  2.66651035e-09 5.55795680e-07]\n",
            " [1.70210413e-07 9.99987483e-01 9.98786390e-07 4.16276658e-09\n",
            "  3.28663350e-06 1.28371340e-07 7.35709138e-09 7.04025615e-06\n",
            "  5.58876593e-07 3.78106506e-07]\n",
            " [2.75764603e-14 1.16063904e-06 2.10424261e-10 2.91065559e-14\n",
            "  9.99995589e-01 2.63445186e-11 4.23980593e-13 5.04336811e-07\n",
            "  2.04554766e-07 2.54577253e-06]\n",
            " [1.50671248e-15 7.76028020e-10 3.07625071e-11 3.38521185e-15\n",
            "  1.07320625e-06 8.96857594e-14 9.14781183e-16 9.93575291e-11\n",
            "  1.08101805e-10 9.99998927e-01]\n",
            " [1.90949294e-07 7.02314422e-12 3.77814490e-12 8.23583868e-09\n",
            "  6.91064206e-10 9.96396244e-01 1.14537752e-03 1.19672284e-11\n",
            "  2.45754747e-03 5.63723972e-07]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_14'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "    vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tvptcn8dxvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}